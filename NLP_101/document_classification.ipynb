{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "7fb69e8d51626fcdcaaed8a841f4586d855c89eec17d0eb311df0b293d35962a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# http://qwone.com/~jason/20Newsgroups/\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# show categories\n",
    "twenty_train.target_names[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\",\n",
       " 'Subject: WHAT car is this!?',\n",
       " 'Nntp-Posting-Host: rac3.wam.umd.edu',\n",
       " 'Organization: University of Maryland, College Park',\n",
       " 'Lines: 15',\n",
       " '',\n",
       " ' I was wondering if anyone out there could enlighten me on this car I saw',\n",
       " 'the other day. It was a 2-door sports car, looked to be from the late 60s/',\n",
       " 'early 70s. It was called a Bricklin. The doors were really small. In addition,',\n",
       " 'the front bumper was separate from the rest of the body. This is ',\n",
       " 'all I know. If anyone can tellme a model name, engine specs, years',\n",
       " 'of production, where this car is made, history, or whatever info you',\n",
       " 'have on this funky looking car, please e-mail.',\n",
       " '',\n",
       " 'Thanks,',\n",
       " '- IL',\n",
       " '   ---- brought to you by your neighborhood Lerxst ----',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# check out first data\n",
    "twenty_train.data[0].split('\\n')"
   ]
  },
  {
   "source": [
    "### bag of words (all the words from all the docs)\n",
    "### https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "### features are all words in all documents \n",
    "### TF is the count of each word per doc \n",
    "### shape is #docs (n_samples), #words (n_features)\n",
    "### TF-IDF \n",
    "### https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmcda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmcda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(['stopwords','punkt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a classifier using naive bayes\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                      ('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "                    \n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)      \n",
    "\n",
    "# test, predict,  display accurracy\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8240839086563994"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# make a classifier using support vector machine (SVM)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "text_clf_svm = Pipeline([\n",
    "                            ('vect', CountVectorizer()),\n",
    "                            ('tfidf', TfidfTransformer()),\n",
    "                            ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)),\n",
    "                        ])\n",
    "text_clf_svm.n_iter=5\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# test, predict,  display accurracy\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search to find best hyper params for NB model\n",
    "\n",
    "parameters = {\n",
    "                'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                'tfidf__use_idf': (True, False),\n",
    "                'clf__alpha': (1e-2, 1e-3),\n",
    "            }\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9051618841994754\n{'clf-svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find best hyper params for SVM model\n",
    "\n",
    "parameters_svm = {\n",
    "                    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                    'tfidf__use_idf': (True, False),\n",
    "                    'clf-svm__alpha': (1e-2, 1e-3),\n",
    "                }\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\mmcda\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", '``', 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8215613382899628"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([\n",
    "                            ('vect', TfidfVectorizer(tokenizer=stemming_tokenizer\n",
    "                                    , stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "                            ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)),\n",
    "                        ])\n",
    "text_clf_svm.n_iter=5\n",
    "\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "# test, predict,  display accurracy\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\mmcda\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", '``', 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8324482209240573"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                      ('vect', TfidfVectorizer(tokenizer=stemming_tokenizer\n",
    "                                    , stop_words=stopwords.words('english') + list(string.punctuation), min_df=5)),\n",
    "                      ('clf', MultinomialNB(alpha=0.005)),\n",
    "                    ])\n",
    "                    \n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)      \n",
    "\n",
    "# test, predict,  display accurracy\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}